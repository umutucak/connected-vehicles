{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyA-BjlFgaqa"
   },
   "source": [
    "# Connected and Autonomous Driving - Visual Perception Module\n",
    "Lab credits: [FranÃ§ois Robinet](mailto:francois.robinet@uni.lu)\n",
    "\n",
    "----\n",
    "\n",
    "This assignement is separated in 3 parts related to visual percepetion: \n",
    "- In the first, you will learn about creating an image classification model from nothing.\n",
    "- In the second, you will use and fine-tune a detection model.\n",
    "- In the third, you will learn how to use this model in a simulation environment.\n",
    "\n",
    "In this assignment, we will learn how to tackle the complex problem of classifying traffic signs. These signs come in many shapes, colors and forms, and it is hard even for us to express how exactly we identify traffic signs. This makes learning from actual data an ideal fit for this problem!\n",
    "\n",
    "The following concepts will be covered in this Lab:\n",
    "\n",
    "1. Loading data in a format suitable for learning algorithms\n",
    "1. Implementation of a Softmax Classifier in plain PyTorch\n",
    "1. Performance assessment in machine learning\n",
    "1. GPU Implementation of complex classification models using PyTorch\n",
    "1. Regularization in the context of neural networks (data augmentation, dropout, early stopping)\n",
    "1. Fine-tuning a pre-trained detection model to fit an application\n",
    "\n",
    "In this lab, we will leverage the GPU capabilities of Google Colab. To enable a GPU for this notebook, go to \"Execution > Change Execution Type\" and add GPU capabilities. Executing the next cell should succeed and show one available GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzvBosbGHhGQ",
    "outputId": "982af908-998c-4c67-8a65-6ffafb8cfd33"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMLiTcpuh2yU"
   },
   "source": [
    "## Loading Traffic Sign Data\n",
    "\n",
    "You will find the traffic sign dataset used for this assignment `traffic_signs.zip` in the git page of this project.\n",
    "\n",
    "download the corresponding zip file and upload it to the repository of this google collab, then run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EsFY-RO7suS",
    "outputId": "8c4e6f64-e1b2-4410-a3da-9b10a0ef867e"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FILENAME=\"traffic_signs.zip\"\n",
    "traffic_signs_dir = Path(\"/content\") / \"datasets/traffic_signs_data\"\n",
    "traffic_signs_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Extracting the traffic sign dataset\")\n",
    "! unzip $FILENAME -d $traffic_signs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbBZy_4m3BCM"
   },
   "source": [
    "The traffic sign data is split into train / validation / test datasets.\n",
    "Each of this dataset has can be loaded directly using `numpy.load`.\n",
    "\n",
    "The data contains the following data for the traffic signs:\n",
    "\n",
    "- `features`: the traffic sign 32x32 3-channel RGB images\n",
    "- `labels`: a single label for each sample, represented as a number in [0,42]. These numbers relate to each of the 43 traffic signs names that are read from  `signnames.csv`.\n",
    "\n",
    "Let's load the 34799 samples of the training set and display a 16 random images with their labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "AjGW2uPi1Y39",
    "outputId": "0e2f59db-f1af-46db-f8da-ca3d9aac52aa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def load_data(path):\n",
    "  data = np.load(path, allow_pickle=True)\n",
    "  images, labels = data[\"features\"], data[\"labels\"]\n",
    "  # Pixels are stored as unsigned integers in [0,255]\n",
    "  # Because our numerical operations treat these as floating point numbers,\n",
    "  # it's best to convert these numbers to float and map them to [0,1]\n",
    "  images = images.astype(np.float32) / 255.0\n",
    "  labels = labels.astype(np.int64)\n",
    "  return images, labels\n",
    "\n",
    "def show_image_grid(images, titles):\n",
    "  fig = plt.figure(figsize=(16,16))\n",
    "  ncols = 4\n",
    "  nrows = int(np.ceil(images.shape[0] // ncols))\n",
    "  grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=0.75)\n",
    "  for ax, img, title in zip(grid, images, titles):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Read data\n",
    "sign_names = pd.read_csv(traffic_signs_dir / \"signnames.csv\").SignName.values\n",
    "training_images, training_labels = load_data(traffic_signs_dir / \"train.p\")\n",
    "training_size = training_labels.shape[0]\n",
    "\n",
    "# Display 16 random images with their labels\n",
    "random_indices = np.random.choice(np.arange(training_size), size=16, replace=False)\n",
    "show_image_grid(training_images[random_indices], sign_names[training_labels[random_indices]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDjxZhdm8abl"
   },
   "source": [
    "## Classification with a Single Layer: Logistic/Softmax Regression\n",
    "\n",
    "### Model\n",
    "\n",
    "Our first approach will be to implement a simple linear classifier using numpy only. This means that our output will be predicted as\n",
    "\n",
    "$$\\hat{y} = \\sigma(Wx+b)$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x$ is a vector image pixel values. Shape is $(3*32*32,) = (3072,)$.\n",
    "- $\\hat{y}$ is a $(43,)$ shaped vector containing a probability value for each class.\n",
    "- $W$ is a (43,3072) matrix of weights to be learned. The weights will be initialized to random numbers in $[0,1]$. More advanced strategies exist and will be covered in the next parts.\n",
    "- $b$ is a vector of weights called the biases, that will also be learned. Its shape is $(43,)$\n",
    "- $\\sigma$ is the softmax activation function which will project values in $Wx$ into $[0,1]$ and make them sum to one: $\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_je^{x_j}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5A1iyqvzlxm"
   },
   "source": [
    "\n",
    "### From Theory to Practice: Implementing the model\n",
    "\n",
    "Let's now implement our softmax classifier in PyTorch. We can easily do so by creating a new class deriving from `torch.nn.Module` and implementing the computation in its `forward` method. You'll learn more about why this is called `forward` next week.\n",
    "\n",
    "If you want to show the documentation relative to a function like `torch.nn.Parameter`, just execute the following code in a cell and documentation will pop-up.\n",
    "\n",
    "```\n",
    "torch.nn.Parameter?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "491rJZPNDPG2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__() # Mandatory: PyTorch does some setting up in there!\n",
    "    # Create our weight Tensor W. It will be of type float and have shape (1,43,3*32*32)\n",
    "    # The extra first dimension is there because nn.Modules are meant to process\n",
    "    # batches of inputs, and the inputs after reshaping will have shape (B, 3072, 1),\n",
    "    # where B is the batch size. Adding an extra dimension is needed for numpy\n",
    "    # to broadcast the first dimension of our weights to B.\n",
    "    W_shape = (1, 43, 3072) # TODO\n",
    "    bias_shape = (1, 1, 3072) # TODO\n",
    "    # Since we want to learn these weights, we will create them as \"parameters\" (another name for weights)\n",
    "    # Initialize parameters with random numbers in [0,1] (biases can be initialized to zero)\n",
    "    # Better init stategies exist and will be covered later on\n",
    "    self.W = nn.Parameter(torch.Tensor(*W_shape))\n",
    "    nn.init.uniform_(self.W, 0.0, 1.0)\n",
    "    self.bias = nn.Parameter(torch.zeros(bias_shape))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # The formula treats x as a vector, but we're receiving a batch of images\n",
    "    # with shape (B,32,32,3), which we reshape to (B,32*32*3,1).\n",
    "    x = x.view(x.shape[0],-1,1)\n",
    "    # TODO: Compute our prediction y_hat following the formula in the description above\n",
    "    # Hint: the softmax function is available as torch.nn.functional.softmax (here F.softmax)\n",
    "    # Keep in mind that you want to compute the softmax separately on each image,\n",
    "    # so no softmax on dim 0!\n",
    "    y_hat = None # TODO\n",
    "    return torch.squeeze(y_hat) # Squeeze removes any extra dimension with value 1, so we're back to (B,43) shape\n",
    "\n",
    "# Create our classifier\n",
    "softmax_classifier = SoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpiibR4SzDhX"
   },
   "source": [
    "### Our very first prediction\n",
    "\n",
    "We have a randomly intialized model, let's use it to make a prediction on a random batch of images!\n",
    "\n",
    "Remember that we are predicting 43 numbers for each image, which according to our model can be interpreted as the probability of each class to be the \"right one\". To get an actual prediction, we can simply grab the class with maximal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qc4yYzbJy_iJ"
   },
   "outputs": [],
   "source": [
    "# We will reuse the random batch of 16 images we showed when loading the data\n",
    "input_images = torch.from_numpy(training_images[random_indices]) # This has shape (16,32,32,3)\n",
    "with torch.no_grad(): # Don't worry about no_grad for now, it just means we're not trying to optimize anything\n",
    "  # TODO: Compute class probabilities by feeding the random_batch to your model (just a function call like \"model(x)\")\n",
    "  y_pred = None # Class probabilities\n",
    "  assert y_pred.shape == (input_images.shape[0],sign_names.shape[0]), f\"Predicted shape should be {(input_images.shape[0],sign_names.shape[0])} and is {y_pred.shape}.\"\n",
    "\n",
    "# TODO: Find the most probable label for each image according to our model\n",
    "# Hint: torch.argmax should help you there, just keep in mind that you want the max *per-image* (so don't argmax on dimension 0)\n",
    "predicted_labels = None # Most probable label for each image\n",
    "assert predicted_labels.shape == (input_images.shape[0],), f\"Predicted labels shape should be {(input_images.shape[0],)} and is {(input_images.shape[0],)}\"\n",
    "predicted_sign_names = sign_names[predicted_labels]\n",
    "\n",
    "# Show the image and their predicted class\n",
    "show_image_grid(input_images, predicted_sign_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_29NU86YzeH-"
   },
   "source": [
    "### Training our Model\n",
    "\n",
    "Our first prediction is rather... disappointing. It's not surprising though: we've learned a lot, but our model hasn't and it's predicting at random, often choosing the same class for most inputs! Let's see if we can fix this!\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "We first transform each ground truth label into a one-hot encoded vector\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  y_i=\\begin{cases}\n",
    "    1 & \\text{if sample has class } i\\\\\n",
    "    0 & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We want to make $\\hat{y}$ close to the ground truth $y$, which can be done in several ways. For multinomial classification, it is very common to use the \"Negative Log-Likelihood Loss\" we covered in class. In PyTorch, this loss function $L(y,\\hat{y})$ is implemented in `torch.nn.NLLLoss`.\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent iteratively updates the weights in order to minimize $L(\\hat{y},y)$. Remember that $L$ depends on the weights because each prediction $\\hat{y} = \\sigma(Wx+b)$ depends on them.\n",
    "\n",
    "We seek to compute the optimal weights $W^*$ such that\n",
    "\n",
    "$$W^* = \\underset{W,b}{\\mathrm{argmin}}\\ L(\\sigma(Wx+b),y)$$\n",
    "\n",
    "Gradient-based optimization iteratively improves on weight estimates $(W_k, W_{k+1}, ...)$ and $(b_k, b_{k+1}, ...)$ by making steps in the direction opposite to the gradient/jacobian directions\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "    W_{k+1} &= W_k - \\alpha \\nabla_W L\\\\\n",
    "    b_{k+1} &= b_k - \\alpha \\nabla_b L\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\alpha$ defines a \"step size\", usually refered to as \"learning rate\" in machine learning. There are also more fancy ways to update parameters at each iteration, and we will discuss these in later classes.\n",
    "\n",
    "Computing $\\nabla_W L$ and $\\nabla_b L$ is not trivial and will be covered in later classes.\n",
    "One of the primary reasons for using Deep Learning frameworks like PyTorch or Tensorflow is that they will compute these for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0SDlBem6sMZ"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # We'll use this for nice progress bars\n",
    "\n",
    "# Training Loop:\n",
    "#   - separate the training data into batches of 64 samples\n",
    "#   - each epoch goes over all the batches in the dataset, and we can repeat the process for many epochs\n",
    "#   - we feed each batch to our model, compute a loss, compute its gradient, and perform an SGD update\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-3\n",
    "batches_per_epoch = int(np.ceil(training_images.shape[0] / batch_size))\n",
    "loss_function = torch.nn.NLLLoss() # Negative Log-Likelihood loss function (aka cross-entropy loss)\n",
    "\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "softmax_classifier.train()\n",
    "epoch_losses = []\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "  average_epoch_loss = 0 # Running average loss over epoch\n",
    "  for batch_idx in range(batches_per_epoch):\n",
    "    # Get batch of training data\n",
    "    batch_start_index = batch_size * batch_idx\n",
    "    batch_images = torch.from_numpy(training_images[batch_start_index:batch_start_index+batch_size])\n",
    "    batch_labels = torch.from_numpy(training_labels[batch_start_index:batch_start_index+batch_size])\n",
    "\n",
    "    # TODO: Feed images to the model\n",
    "    class_probabilities = softmax_classifier(batch_images)\n",
    "\n",
    "    # Compute loss value using NLLLoss\n",
    "    # NLLLoss expects log-probabilities rather than probabilities\n",
    "    # NLLLoss will also handle converting ground truth labels to one-hot encoded vectors\n",
    "    class_log_probabilities = torch.log(class_probabilities+1e-7) # 1e-7 added for numerical stability\n",
    "    loss = loss_function(class_log_probabilities, batch_labels)\n",
    "\n",
    "    # Print loss information so we can monitor training\n",
    "    average_epoch_loss = (batch_idx * average_epoch_loss + loss.item()) / (batch_idx+1)\n",
    "\n",
    "    # Compute the gradient of the loss wrt. to model parameters\n",
    "    loss.backward() # The reason why this is called \"backward\" will become clear after the next class\n",
    "\n",
    "    # TODO: Implement SGD update\n",
    "    # After the call to loss.backward, the gradient of the loss with respect to parameter w is `w.grad`\n",
    "    # You can access all the model's parameters with `softmax_classifier.parameters()` and loop over them to update them\n",
    "    with torch.no_grad(): # Loop wrapped in no_grad again because we don't need to accumulate gradients for this step\n",
    "      for p in softmax_classifier.parameters():\n",
    "        p = None # TODO SGD Update\n",
    "        # *important*: by default PyTorch remembers gradients from previous iterations, so we always have to zero them out before next iteration\n",
    "        p.grad.zero_()\n",
    "  epoch_losses.append(average_epoch_loss)\n",
    "  pbar.set_description(f\"Last Epoch Loss: {average_epoch_loss:.4f}\")\n",
    "\n",
    "# Plot training loss evolution\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmQFZ7P0-IUe"
   },
   "source": [
    "### CUDA to the Rescue\n",
    "\n",
    "Our model is learning, but it's pretty slow, taking upward of 15s for each epoch. We can speed this up tremendously by leveraging the GPU. Indeed, Matrix multiplication is at the heart of our model, and is the type of heavily parallelizable operation that GPUs are extremely efficient at.\n",
    "\n",
    "PyTorch makes moving this kind of computation to the GPU extremely easy. Modify the above code cell to\n",
    "\n",
    "- Move your model to GPU memory: `softmax_classifier = SoftmaxClassifier().cuda()`\n",
    "- Move your inputs to GPU memory: `batch_images = batch_images.cuda()`, `batch_labels = batch_labels.cuda()`\n",
    "- Crank up the number of epochs: `num_epochs = 100`\n",
    "\n",
    "To bring a GPU Tensor back to CPU RAM, we can simply use `tensor.cpu()`.\n",
    "\n",
    "Run again, and enjoy the free speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzxSmTepXLvA"
   },
   "source": [
    "## Assessing Multinomial Classification Models\n",
    "\n",
    "### Why bother with a test set?\n",
    "\n",
    "The training loss informs us that the model is training, but it does not give us much information about the inference performance of that model. Indeed training metrics have no value when assessing generalization performance. To covince yourself of that fact, imagine a model that simply memorizes the training set entirely. This model is trivial to implement, it will have 0 training loss, but will be utterly useless to make predictions on unseen data!\n",
    "\n",
    "To assess the generalization abilities, one should use a separate split of the data called the test set. This split of the data must never be used during training, and can only be used after the fact in order to estimate the performance of the trained model on unseen data. As much as possible, this test set should be representative of the data that would be encountered in the real-world application. This is also true for the training set of course!\n",
    "\n",
    "Let's start assessing our model by computing the simplest classification metric: accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xnz4nth1ZGFN"
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, Callable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def eval_metrics(model, dataset_images, dataset_labels, batch_size, metrics_fns: Dict[str,Callable], desc=None):\n",
    "  model.eval()\n",
    "  total_batches = int(np.ceil(dataset_images.shape[0] / batch_size))\n",
    "  with torch.no_grad():\n",
    "    pred_labels = []; true_labels = []\n",
    "    for batch_idx in tqdm(range(total_batches), desc=desc):\n",
    "      batch_start_index = batch_idx * batch_size\n",
    "      batch_images = torch.from_numpy(dataset_images[batch_start_index:batch_start_index+batch_size]).cuda()\n",
    "      batch_labels = torch.from_numpy(dataset_labels[batch_start_index:batch_start_index+batch_size]).cuda()\n",
    "      class_probabilities = model(batch_images)\n",
    "      predicted_labels = torch.argmax(class_probabilities, dim=1)\n",
    "      pred_labels.append(predicted_labels.cpu().numpy())\n",
    "      true_labels.append(batch_labels.cpu().numpy())\n",
    "  pred_labels = np.hstack(pred_labels)\n",
    "  true_labels = np.hstack(true_labels)\n",
    "  return { name: metric(true_labels, pred_labels) for name, metric in metrics_fns.items() }\n",
    "\n",
    "test_images, test_labels = load_data(traffic_signs_dir / \"test.p\")\n",
    "training_metrics = eval_metrics(softmax_classifier, training_images, training_labels, 256, {\"accuracy\": accuracy_score}, desc=\"Training Eval\")\n",
    "test_metrics = eval_metrics(softmax_classifier, test_images, test_labels, 256, {\"accuracy\": accuracy_score}, desc=\"Test Eval\")\n",
    "print(f\"Training metrics: {training_metrics}\")\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "The6aP5jcx6Q"
   },
   "source": [
    "### So, are we done yet?\n",
    "\n",
    "You should observe a test set accuracy around 40%, and an even better  one for the training set. If the gap between the training and test set is large, this indicates overfitting and a need for regularization.\n",
    "\n",
    "We should note that, while a test set accuracy of 40% is not great, it is also pretty impressive considering the simplicity of our model. With 43 classes, and a model predicting completely at random would only achieve $\\frac{1}{43} \\approx 2.5\\%$ accuracy, so we have that going for us...\n",
    "\n",
    "<img src=\"https://i.imgflip.com/4kb85r.jpg\" align=\"center\" />\n",
    "\n",
    "\n",
    "### Multi-class Classification Metrics\n",
    "\n",
    "The following cell shows a distribution of classes in the test set. As expected for traffic signs, the classes are heavily imbalanced: some categories are much more common than others.\n",
    "\n",
    "If our goal is to be able to detect all traffic signs with the same precision, we should not look at plain accuracy results, but rather at metrics that take imbalance into account, such as the balanced accuracy. For details on other possible metrics, see [this survey](https://arxiv.org/pdf/2008.05756.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0iltL4sXJWb"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o8wRDU4g0n9"
   },
   "outputs": [],
   "source": [
    "# Balanced accuracy computation\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score\n",
    "}\n",
    "training_metrics = eval_metrics(softmax_classifier, training_images, training_labels, 256, metrics, desc=\"Training Eval\")\n",
    "test_metrics = eval_metrics(softmax_classifier, test_images, test_labels, 256, metrics, desc=\"Test Eval\")\n",
    "print(f\"Training metrics: {training_metrics}\")\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0CBfj74hnDm"
   },
   "source": [
    "### What now?\n",
    "\n",
    "They are two possibilities to go forward:\n",
    "\n",
    "1. We trust in our current model, and we decide to fine-tune hyperparameters such as the batch size or the learning rate. This will require the use of a validation set to find optimal parameters.\n",
    "2. We decide that a linear classifier acting directly on pixels is probably not that great of an idea to begin with, and we decide to go deeper!\n",
    "\n",
    "Guess what, we'll go for option 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUWxv9SMiIGi"
   },
   "source": [
    "## Going Deeper: Multi-Layer Classifier\n",
    "\n",
    "In this section, we'll redo much of the same work as for the single layer classification but, now that we know how things work under the hood, we'll rely a lot more on PyTorch to do things for us!\n",
    "\n",
    "Our first model was nothing else than a softmax activation function stacked on top of a linear layer. The next one will exploit the same idea by stacking multiple layers on top.\n",
    "\n",
    "The architecture will be as follows:\n",
    "\n",
    "- Input $x$ of shape (B,32,32,3)\n",
    "- Layer 1: Linear layer (aka fully-connected) with 100 neurons with biases, followed by ReLU activation ($32*32*3*100 + 100$ learnable weights)\n",
    "- Layer 2: Linear layer with 75 neurons with biases, followed by ReLU activation ($100*75+75$ learnable weights)\n",
    "- Output Layer: Linear layer with 43 neurons with biases followed by softmax activation ($75*43+43$ learnable weights)\n",
    "\n",
    "Where $B$ is the batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdS11pdviXak"
   },
   "source": [
    "\n",
    "### PyTorch, batteries included\n",
    "\n",
    "This time around, we will be using PyTorch's built-in layers, instead of creating them by hand like the first time. For reference, we could have simply implemented our previous model as:\n",
    "\n",
    "```\n",
    "softmax_classifier = nn.Sequential(nn.Flatten(), nn.Linear(32*32*3, 43), nn.Softmax())\n",
    "```\n",
    "\n",
    "We will also keep relying on the GPU for our model's computation using `.cuda()` to move tensor to the GPU.\n",
    "\n",
    "Start the following cell to start training the model, and take your time to go over the code. There are no new concept, but this is now implemented using more of PyTorch's utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07ze7vJbiVxz"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch has utilities for dealing with datasets that extend its Dataset class\n",
    "# It will be able to automatically load in batches and in parallel.\n",
    "# In this toy example, the whole dataset fits in memory, but in most application,\n",
    "# the whole dataset can weight 100GB+, so loading it in parallel makes a big difference.\n",
    "class TrafficSignsDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, path):\n",
    "    images, labels = load_data(path)\n",
    "    images = images.transpose((0,3,1,2)) # PyTorch expects image data in (channel,height,width) format instead of the usual (height,width,channel)\n",
    "    self.images = torch.from_numpy(images).cuda() # Send data to GPU memory\n",
    "    self.labels = torch.from_numpy(labels).cuda()\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Training loop\n",
    "batch_size = 256\n",
    "training_dataset = TrafficSignsDataset(traffic_signs_dir / \"train.p\")\n",
    "validation_dataset = TrafficSignsDataset(traffic_signs_dir / \"valid.p\")\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def predict_batch(model, batch_images, no_grad: bool):\n",
    "  if no_grad:\n",
    "    with torch.no_grad():\n",
    "      class_probs = model(batch_images)\n",
    "  else:\n",
    "      class_probs = model(batch_images)\n",
    "  class_labels = torch.argmax(class_probs, dim=1)\n",
    "  return class_probs, class_labels\n",
    "\n",
    "def eval_metrics(pred_labels, true_labels, prefix):\n",
    "    metrics = {\n",
    "      f\"{prefix}accuracy\": accuracy_score,\n",
    "      f\"{prefix}balanced_accuracy\": balanced_accuracy_score\n",
    "    }\n",
    "    return { name: metric_fn(true_labels,pred_labels) for name, metric_fn in metrics.items() }\n",
    "\n",
    "def train_model(model, training_loader, validation_loader, num_epochs, learning_rate):\n",
    "  # Loss, optimizer and metrics\n",
    "  loss_function = torch.nn.NLLLoss() # Negative Log-Likelihood loss function\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # We'll use a built-in optimizer implementation this time\n",
    "  pbar = tqdm(range(num_epochs),desc=\"Epochs\")\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    train_true_labels = []\n",
    "    train_pred_labels = []\n",
    "    for batch_idx, (batch_images, batch_labels) in enumerate(training_loader):\n",
    "      # Loss computation\n",
    "      class_probs, class_labels = predict_batch(model, batch_images, no_grad=False)\n",
    "      class_log_probs = torch.log(class_probs+1e-7)\n",
    "      train_true_labels.append(batch_labels.cpu().numpy())\n",
    "      train_pred_labels.append(class_labels.cpu().numpy())\n",
    "      loss = loss_function(class_log_probs, batch_labels)\n",
    "      train_losses.append(loss.item())\n",
    "      # SGD Step\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    # Evaluation\n",
    "    # Because Google lets us use pretty big GPUs for free,\n",
    "    # we can predict the whole validation set at once!\n",
    "    # This if of course almost never possible on larger datasets and in practice\n",
    "    # we'd have to use a validation dataloader like for the training set\n",
    "    _, val_class_labels = predict_batch(model, validation_dataset.images, no_grad=True)\n",
    "    val_metrics = eval_metrics(\n",
    "        val_class_labels.cpu().numpy(), # Send results GPU -> CPU memory\n",
    "        validation_dataset.labels.cpu().numpy(),\n",
    "        prefix=\"val_\")\n",
    "    train_metrics = eval_metrics(\n",
    "        np.hstack(train_pred_labels),\n",
    "        np.hstack(train_true_labels),\n",
    "        prefix=\"train_\")\n",
    "    train_metrics[\"train_loss\"] = np.mean(train_losses)\n",
    "    pbar.set_postfix({**train_metrics, **val_metrics})\n",
    "\n",
    "# Define the model\n",
    "deep_softmax_classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*32*3, 100), nn.ReLU(),\n",
    "    nn.Linear(100, 75), nn.ReLU(),\n",
    "    nn.Linear(75, 43), nn.Softmax(dim=1),\n",
    ").cuda() # Send model to GPU memory\n",
    "\n",
    "# Start training\n",
    "#train_model(deep_softmax_classifier, training_loader, validation_loader, num_epochs=150, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esKFcpkZHBJU"
   },
   "source": [
    "The validation accuracy should now be closer to 85%, which is a nice improvement! You're also probably observing a much higher training accuracy, which is a sign of overfitting!\n",
    "\n",
    "Let's see if we can do even better if we stop considering all pixels independently: enter convolutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3CDP7zwH9i_"
   },
   "source": [
    "## Making things more Convoluted\n",
    "\n",
    "Now that we are familiar with Linear layers, it's time to discover another type of layer: the extremely popular Convolutional Layer.\n",
    "\n",
    "We will use these layers as a \"feature extractor\":\n",
    "\n",
    "- Instead of using `Flatten` directly on the input pixels like we've done so far, these pixels will first be pre-processed by a few Conv layers, and we will flatten their output to a (B,512) vector\n",
    "- The next layers will be the usual Linear layers, the only difference is that they don't operate on raw pixels anymore.\n",
    "\n",
    "You can start out by adding 2 Linear layers and train the model. You should get very good training performance, but your model will likely overfit. Your job for assignment 1 will be to tweak both the architecture and the learning strategy in order to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xb1JXqyF56l"
   },
   "outputs": [],
   "source": [
    "# A typical CNN classifier\n",
    "# We will discuss how this works in next week's class.\n",
    "# For this assignment, use it as-is\n",
    "# If you know Convolutional layers already, feel free to tweak it as much as you want\n",
    "# but it's not necessary to complete the assignment!\n",
    "feature_extractor = nn.Sequential(\n",
    "    nn.Conv2d(3, 24, kernel_size=(6,6)), nn.ReLU(),\n",
    "    nn.Conv2d(24, 36, kernel_size=(6,6)), nn.ReLU(),\n",
    "    nn.Conv2d(36, 48, kernel_size=(4,4), stride=2), nn.ReLU(),\n",
    "    nn.Conv2d(48, 64, kernel_size=(4,4), stride=2), nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=(2,2), stride=2), nn.ReLU(),\n",
    ")\n",
    "\n",
    "conv_classifier = nn.Sequential(\n",
    "    feature_extractor, # Extracts useful features\n",
    "    nn.Flatten(), # Flatten all features into a single vector of dim (B,512)\n",
    "    # TODO: Your layers go here\n",
    ").cuda() # Send model to GPU memory\n",
    "\n",
    "# Start training\n",
    "train_model(conv_classifier, training_loader, validation_loader, num_epochs=100, learning_rate=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQx6zsn5IjMA"
   },
   "source": [
    "## Assignment (Part One - Classification)\n",
    "\n",
    "Your assignment is to build on the last model to further improve\n",
    "performance. There are several ways you can improve what we have done so far:\n",
    "\n",
    "- **Model Architecture**: Add more layers or remove some. If you're not familiar with Convolutional Layers, leave the feature extractor part untouched.\n",
    "- **Regularization**: Early Stopping, Dropout layers, Batch Normalization, ...\n",
    "- **Data Augmentation**: Dynamically modify training inputs in ways that don't change the output class that should be predicted, so that the classifier learns to deal with variations in the input. There are hundreds of ways to proceed, eg. by changing contrast, lightly blurring images, flipping them horizontally, taking crops, adding light noise on them, ... Be creative! A combination of search engine use and the `transform` module of the `torchvision` library might be very helpful for this. You can implement this in a flexible way by simply adding an `augment(x)` functiont that applies a random augmentation on a batch `x` and call it inside the training loop.\n",
    "- **Hyperparameter Tuning**: Try to tweak the learning rate and batch size, using the validation set performance to guide your search. You can also experiment with the parameters of the SGD optimizer, or even try different optimizers altogether.\n",
    "\n",
    "You should aim the best possible accuracy on the test set (95%+ is a good milestone). Of course, you should not use performance on the test set to tune your model, only the validation set. The test set should only be touched once you're satisfied of the performance of your model on the training and validation sets.\n",
    "\n",
    "Once you're satisfied with your model, **answer the following questions by adding code and text cells below these instructions.**\n",
    "\n",
    "- Show at least 5 test samples where your model guesses the wrong class. What are the 5 most probable classes according to your model for these examples?\n",
    "- Are there classification mistakes that are more common than others? To assess this, plot a [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html). Are these mistakes similar to mistakes a human could do?\n",
    "- Show the learning curves (loss values evolution with training epochs) on training and validation sets. What do these curve tell you?\n",
    "\n",
    "Once you're ready to submit, create an individual code cell at the end of this notebook. This cell should be self-contained and create an instance of your model, restore your saved weights, and make predictions on the test set. For instructions on how to save/restore weights, see the [PyTorch Documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "At the end of this module, send this model's weight alongside the rest of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msj60ItQRtzI"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment (Part two - Fine-tuning)\n",
    "\n",
    "Now that we have successfully built our own CNN, we made a good image classification model, but we can still make it better. Moreover, for the perception module of an autonomous vehicle, the application we are aiming for is not classification, but detection. We don't want to classify every frame 'seen' by our vehicle, but we want to detect important objects around us, like cars or traffic lights.\n",
    "\n",
    "We will use the state of the art by importing a pre-trained YOLO model, and fine-tune it on a CARLA dataset.\n",
    "\n",
    "Let's start by using the model directly off-the-shelf and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO model \n",
    "model = YOLO(\"yolo11m.pt\")\n",
    "# There are 5 different versions of YOLO11 which we can use, from n to x. Yolo11n is smaller and faster, but also less precise.\n",
    "# Yolo11x is bigger and more powerful, but also more ressource expensive.\n",
    "#   -yolo11n\n",
    "#   -yolo11s\n",
    "#   -yolo11m\n",
    "#   -yolo11l\n",
    "#   -yolo11x\n",
    "# yolo11m is a good compromise between speed and precision. \n",
    "\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val(data=\"coco8.yaml\")\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "\n",
    "#Here are a few useful methods to get the results of your model.\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try importing some of the other YOLO models in the cell above. What changes in the output can you see between different versions ?\n",
    "- How many parameters are there in Yolo11n ? Yolo11x ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained YOLO11 model was trained on the COCO dataset containing real images, which reduce its performance when applied to our simulation scenario. To improve it, we want to fine-tune it on a CARLA dataset.\n",
    "\n",
    "Moreover, the YOLO11 model that we are using is trained on 80 different classes. Some of them are very useful for us like detecting cars, bus and traffic lights but some are not relevant for us. Most importantly, there are classes which we want to detect that are not present in the original dataset used to train YOLO, like traffic light colors !\n",
    "\n",
    "For our autonmous driving application, we need a dataset containing images collected in the CARLA simulation environment but also labels that reflect the specific things we need to detect. \n",
    "\n",
    "You can find our custom dataset at : https://app.roboflow.com/carla-test/modified-carla/3\n",
    "\n",
    "Download the last version in the YOLO11 format and upload the zip file to your google collab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ModifiedCARLA.v3i.yolov11.zip -d /content/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you notice about the label distribution ?\n",
    "- In you opinion, could this be an issue for our application ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on our traffic light dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save path to our dataset. If nothing is specified, utltralytics loads the coco8 dataset\n",
    "path = \"datasets/data.yaml\"\n",
    "\n",
    "# Train the model for a few epochs\n",
    "results = model.train(data=path, epochs=3, warmup_epochs=0, pretrained=True)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val()\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
    "\n",
    "# Save the model\n",
    "success = model.save(\"./yolo_collab_tuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at all the information Ultralytics is giving us:\n",
    "\n",
    "- First, you can see information on the pre-trained model your are using: its architecture, number of layer, parameters...\n",
    "\n",
    "- The line `Overriding model.yaml nc=80 with nc=8` tells us that we are changing the dimension of the final layer of the model with our dataset. While the base Yolo is trained to detect 80 class, we are only using 8.\n",
    "\n",
    "- For each Epoch, you can see the training process advance with the varying loss decreasing over time.\n",
    "\n",
    "- The mAP for each classes as well as the confusion matrix give you very good overview of your performance.\n",
    "\n",
    "You just trained an existing model on our custom dataset. When running the evaluation, Ultralytics provides you with a lot of different metrics in the prompt, and even more which you can find in `run/detect/val_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here\n",
    "\n",
    "#Play around the different training parameters of model.train(), add more epochs, and get the best possible model.\n",
    "#When it starts looking good, import it to CARLA and test it in the simulation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you think the metrics are good enough for your model, you can implement it in CARLA to see if it is able to detect things !\n",
    "Before running your own model in CARLA, When testing how to properly use a model in the simulation, we recommend using the basic YOLO model to verify that you can detect things. Then, compare it with your own to see which one performs best !\n",
    "\n",
    "Your model should be saved by ultralytics in `runs/detect/train_/weights` where you have 2 models saved: best.pt and last.pt. The latter is the last version of the model at the end of training, while the former is the best version of the model during training. We recommend you always use best.pt. Download it and put somewhere you can access easily (like `PythonAPI/Modules/model`) as the rest of the module will be on your local machine and the CARLA server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment (Part three - Detecting objects in real-time and avoiding them)\n",
    "\n",
    "Our fine-tuned Yolo model has been training to recognize objects in CARLA. Now, in order to use it properly in the simulation, there are two important steps left, both in python:\n",
    "\n",
    "- Doing real-time detection of objects\n",
    "- Using detection results to send commands to the car.\n",
    "\n",
    "# Real-time detection\n",
    "\n",
    "Go to `Simulation_Perception.py` and look for the `CameraManager()` class. It is responsible for creating the sensors for the simulation and managing their outputs. \n",
    "This is where you are going to add your model. Look for the `_parse_image(weak_self, image)` that format and sends the raw image from the Carla sensor and you will find a commented section where you can add your model. The objectiv is to do object detection on these frames, save the results and then draw bounding boxes on it before sending it as usual.\n",
    "\n",
    "Once complete, run `Simulation_Perception.py` to see your car detecting objects while moving !\n",
    "don't forget to add the important arguments for the host and the location of your model :\n",
    "`Python Simulation_Perception.py --host 192.168.54.10 -m model/best.pt`\n",
    "\n",
    "If you are not satisfyed with the performance of your model in the simulation you can always come back to Part two and try different technique to improve it ! You can always rely on the base YOLO as well, although it will not allow you to detect traffic light color.\n",
    "\n",
    "# Reacting to detected objects\n",
    "\n",
    "Now, you will work on creating your Custom agent that react to anything your model detects.\n",
    "\n",
    "Go to `Custom_agent.py` and look for the main 2 methods you will be working on: `affected_by_traffic_light()` and `affected_by_vehicle`. Their current version is the default CARLA implementation that relies on ground truth simulation information. \n",
    "\n",
    "Your mission: change the code inside to rely on you detection model instead. You can still use some of the ground truth information, such as the position where the car has to stop for a specific traffic light.\n",
    "The only condition is that your car must react to outside objects IF AND ONLY IF your model has detected it!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
